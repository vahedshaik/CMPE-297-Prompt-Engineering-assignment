# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFOhG4R2KwFQDp-gkfJxxau1gDiT9GPt
"""

!pip install langchain

!pip install pinecone-client

!pip install pypdf

# Define a list of text documents (you should replace this with your actual data)
data = [
    "This is the first document.",
    "Here is the second document.",
    "And this is the third document."
]

# Split the documents into smaller texts
#text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
#texts = text_splitter.split_documents(data)

# Initialize embeddings using OpenAI or your chosen library
# ... Your code for initializing embeddings ...

# Now you can use the 'texts' variable for further processing

from flask import Flask, render_template, request
import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone
from langchain.llms import OpenAI
from langchain.chains.question_answering import load_qa_chain

app = Flask(__name__)

# Load your documents using PyPDFLoader
# Replace the URL with the actual URL of your PDF document
document_url = "https://github.com/vahedshaik/CMPE-297-Prompt-Engineering-assignment/blob/c2645e91e20dbb106bb1434e7b7abae33db9cfd3/gpt%20prompt%20library/templates/ChatGPT_DataScience_Prompts.pdf"
loader = PyPDFLoader(document_url)

# Split the documents into smaller texts
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_documents(loader.load())

# Initialize embeddings using OpenAI
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'sk-')
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

# Initialize Pinecone
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '')
PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')
pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)

# Create a Pinecone index and load the texts
index_name = "langchain"
# Make sure you've created the index previously, dimensions and metric should match your embeddings
docsearch = Pinecone(index_name=index_name)

llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
chain = load_qa_chain(llm, chain_type="stuff")

@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        # Get the user's search prompt from the form
        search_prompt = request.form.get('search_prompt')
        # Get the new prompt from the form
        new_prompt = request.form.get('new_prompt')

        docs = docsearch.query(search_prompt, top_k=10)

        # Add new prompt to the data
        if new_prompt and new_prompt.strip():
            texts.append(new_prompt.strip())
            docsearch.upsert(ids=[str(len(texts) - 1)], vectors=[embeddings.encode(new_prompt.strip())])

        # Perform a similarity search to find prompts similar to the search_prompt
        matching_prompts = set()

        for doc in docs.ids:
            matching_prompts.add(texts[int(doc)])

        matching_prompts_list = list(matching_prompts)
        return render_template('index.html', prompts=matching_prompts_list, search_prompt=search_prompt)

    return render_template('index.html', prompts=None, search_prompt=None)

if __name__ == '__main__':
    app.run(debug=True)
```

Please make sure to replace `'your_openai_api_key_here'` and `'your_pinecone_api_key_here'` with your actual API keys. Additionally, ensure that you've created the Pinecone index with the appropriate dimensions and metric before running this code.









